{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Old"
      ],
      "metadata": {
        "id": "QuHgAY3Zynrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVEprtwhcvXE"
      },
      "outputs": [],
      "source": [
        "# this variable will be used in `runner.train` and by default we disable FP16 mode\n",
        "is_fp16_used = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Catalyst\n",
        "!pip install catalyst==20.12\n",
        "\n",
        "# for augmentations\n",
        "!pip install albumentations==0.4.3\n",
        "\n",
        "# for pretrained segmentation models for PyTorch\n",
        "!pip install segmentation-models-pytorch==0.1.0\n",
        "\n",
        "# for TTA\n",
        "!pip install ttach==0.0.2\n",
        "\n",
        "# for tensorboard\n",
        "!pip install tensorflow\n",
        "\n",
        "# if Your machine support Apex FP16, uncomment this 3 lines below\n",
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
        "# is_fp16_used = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GXeCJtnc3zX",
        "outputId": "fa391a16-f9fc-4072-99aa-dfa36bc1cd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catalyst==20.12\n",
            "  Downloading catalyst-20.12-py2.py3-none-any.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from catalyst==20.12) (6.0)\n",
            "Collecting tensorboardX>=2.1.0\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 73.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.12) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.12) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from catalyst==20.12) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst==20.12) (4.64.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.1.0->catalyst==20.12) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->catalyst==20.12) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->catalyst==20.12) (3.0.9)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3040, in _compute_dependencies\n",
            "    common = frozenset(reqs_for_extra(None))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3037, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 336, in evaluate\n",
            "    return _evaluate_markers(self._markers, current_environment)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 258, in _evaluate_markers\n",
            "    groups[-1].append(_eval_op(lhs_value, op, rhs_value))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/markers.py\", line 209, in _eval_op\n",
            "    return spec.contains(lhs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/specifiers.py\", line 217, in contains\n",
            "    return operator_callable(normalized_item, self.version)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/specifiers.py\", line 329, in wrapped\n",
            "    return fn(self, prospective, spec)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/specifiers.py\", line 508, in _compare_equal\n",
            "    prospective = Version(prospective.public)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 394, in public\n",
            "    return str(self).split(\"+\", 1)[0]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 335, in __str__\n",
            "    parts.append(\".\".join(str(x) for x in self.release))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/version.py\", line 335, in <genexpr>\n",
            "    parts.append(\".\".join(str(x) for x in self.release))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 212, in _main\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1425, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1513, in _log\n",
            "    exc_info, func, extra, sinfo)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1483, in makeRecord\n",
            "    sinfo)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 312, in __init__\n",
            "    self.levelname = getLevelName(level)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 132, in getLevelName\n",
            "    result = _levelToName.get(level)\n",
            "KeyboardInterrupt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting albumentations==0.4.3\n",
            "  Downloading albumentations-0.4.3.tar.gz (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.3) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.3) (1.7.3)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 69.7 MB/s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import catalyst\n",
        "from catalyst import utils\n",
        "\n",
        "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
        "\n",
        "SEED = 42\n",
        "utils.set_global_seed(SEED)\n",
        "utils.prepare_cudnn(deterministic=True)\n"
      ],
      "metadata": {
        "id": "5EKdQ6sXc5RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!download-gdrive 1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj segmentation_data.zip\n"
      ],
      "metadata": {
        "id": "me7hNULzc7J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "IzuhBlPEdrOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/segmentation_data.zip'"
      ],
      "metadata": {
        "id": "KeY2w7hsdn1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"segmentation_data/\")\n",
        "\n",
        "train_image_path = ROOT / \"train\"\n",
        "train_mask_path = ROOT / \"train_masks\"\n",
        "test_image_path = ROOT / \"test\"\n"
      ],
      "metadata": {
        "id": "kWtTtwPLdAlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_IMAGES = sorted(train_image_path.glob(\"*.jpg\"))\n",
        "len(ALL_IMAGES)\n"
      ],
      "metadata": {
        "id": "jTtrS5Z1dBgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_MASKS = sorted(train_mask_path.glob(\"*.gif\"))\n",
        "len(ALL_MASKS)\n",
        "\n"
      ],
      "metadata": {
        "id": "E63ye_Y9dx95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.io import imread as gif_imread\n",
        "from catalyst import utils\n",
        "\n",
        "\n",
        "def show_examples(name: str, image: np.ndarray, mask: np.ndarray):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Image: {name}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask)\n",
        "    plt.title(f\"Mask: {name}\")\n",
        "\n",
        "\n",
        "def show(index: int, images: List[Path], masks: List[Path], transforms=None) -> None:\n",
        "    image_path = images[index]\n",
        "    name = image_path.name\n",
        "\n",
        "    image = utils.imread(image_path)\n",
        "    mask = gif_imread(masks[index])\n",
        "\n",
        "    if transforms is not None:\n",
        "        temp = transforms(image=image, mask=mask)\n",
        "        image = temp[\"image\"]\n",
        "        mask = temp[\"mask\"]\n",
        "\n",
        "    show_examples(name, image, mask)\n",
        "\n",
        "def show_random(images: List[Path], masks: List[Path], transforms=None) -> None:\n",
        "    length = len(images)\n",
        "    index = random.randint(0, length - 1)\n",
        "    show(index, images, masks, transforms)"
      ],
      "metadata": {
        "id": "COt0VfNddz9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "show_random(ALL_IMAGES, ALL_MASKS)\n",
        "\n"
      ],
      "metadata": {
        "id": "vZmQePWGd39E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwIibPZeeCFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original"
      ],
      "metadata": {
        "id": "GPe_2k4Yyo8d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRWGyaT2Y25j"
      },
      "source": [
        "# Catalyst segmentation tutorial\n",
        "\n",
        "Authors: [Roman Tezikov](https://github.com/TezRomacH), [Dmitry Bleklov](https://github.com/Bekovmi), [Sergey Kolesnikov](https://github.com/Scitator)\n",
        "\n",
        "[![Catalyst logo](https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master/pics/catalyst_logo.png)](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "### Colab setup\n",
        "\n",
        "First of all, do not forget to change the runtime type to GPU. <br/>\n",
        "To do so click `Runtime` -> `Change runtime type` -> Select `\"Python 3\"` and `\"GPU\"` -> click `Save`. <br/>\n",
        "After that you can click `Runtime` -> `Run all` and watch the tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHJAs8U5Y25m"
      },
      "source": [
        "\n",
        "## Requirements\n",
        "\n",
        "Download and install the latest versions of catalyst and other libraries required for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxnhRXHoydbP"
      },
      "outputs": [],
      "source": [
        "# this variable will be used in `runner.train` and by default we disable FP16 mode\n",
        "is_fp16_used = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "xCoyLtaeY25m",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Catalyst\n",
        "!pip install catalyst==20.12\n",
        "\n",
        "# for augmentations\n",
        "!pip install albumentations==0.4.3\n",
        "\n",
        "# for pretrained segmentation models for PyTorch\n",
        "!pip install segmentation-models-pytorch==0.1.0\n",
        "\n",
        "# for TTA\n",
        "!pip install ttach==0.0.2\n",
        "\n",
        "# for tensorboard\n",
        "!pip install tensorflow\n",
        "\n",
        "# if Your machine support Apex FP16, uncomment this 3 lines below\n",
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# !pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
        "# is_fp16_used = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVZRDtryydbR"
      },
      "source": [
        "## Setting up GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H65wGVbY25q",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7daa644b-b012-4c5f-e4de-077ccc57935a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 1.12.1+cu113, catalyst: 20.12\n"
          ]
        }
      ],
      "source": [
        "from typing import Callable, List, Tuple\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import catalyst\n",
        "from catalyst import utils\n",
        "\n",
        "print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # \"\" - CPU, \"0\" - 1 GPU, \"0,1\" - MultiGPU\n",
        "\n",
        "SEED = 42\n",
        "utils.set_global_seed(SEED)\n",
        "utils.prepare_cudnn(deterministic=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWseoJqWY25z"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "As a dataset we will take Carvana - binary segmentation for the \"car\" class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34H0dXBYZepr"
      },
      "source": [
        "> If you are on MacOS and you don’t have `wget`, you can install it with: `brew install wget`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pXsc14SydbU"
      },
      "source": [
        "After Catalyst installation, `download-gdrive` function become available to download objects from Google Drive.\n",
        "We use it to download datasets.\n",
        "\n",
        "usage: `download-gdrive {FILE_ID} {FILENAME}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H8tDrZ6Y250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "outputId": "74f0d098-b5e2-4622-d545-f6589767ca7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "--2022-11-15 19:37:54--  https://docs.google.com/uc?export=download&confirm=&id=1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.137.102, 74.125.137.101, 74.125.137.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.137.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2022-11-15 19:37:55 ERROR 404: Not Found.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e6a69f306c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ndownload-gdrive 1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj segmentation_data.zip\\nextract-archive segmentation_data.zip &>/dev/null\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ndownload-gdrive 1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj segmentation_data.zip\\nextract-archive segmentation_data.zip &>/dev/null\\n'' returned non-zero exit status 9."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "download-gdrive 1iYaNijLmzsrMlAdMoUEhhJuo-5bkeAuj segmentation_data.zip\n",
        "extract-archive segmentation_data.zip &>/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4Vqm9FzY254"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"segmentation_data/\")\n",
        "\n",
        "train_image_path = ROOT / \"train\"\n",
        "train_mask_path = ROOT / \"train_masks\"\n",
        "test_image_path = ROOT / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL0sd6__M_98"
      },
      "source": [
        "Collect images and masks into variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU9IPpyAfhOy"
      },
      "outputs": [],
      "source": [
        "ALL_IMAGES = sorted(train_image_path.glob(\"*.jpg\"))\n",
        "len(ALL_IMAGES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElvJQrlOf4Gm"
      },
      "outputs": [],
      "source": [
        "ALL_MASKS = sorted(train_mask_path.glob(\"*.gif\"))\n",
        "len(ALL_MASKS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mcVBd_WjI43"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.io import imread as gif_imread\n",
        "from catalyst import utils\n",
        "\n",
        "\n",
        "def show_examples(name: str, image: np.ndarray, mask: np.ndarray):\n",
        "    plt.figure(figsize=(10, 14))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"Image: {name}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(mask)\n",
        "    plt.title(f\"Mask: {name}\")\n",
        "\n",
        "\n",
        "def show(index: int, images: List[Path], masks: List[Path], transforms=None) -> None:\n",
        "    image_path = images[index]\n",
        "    name = image_path.name\n",
        "\n",
        "    image = utils.imread(image_path)\n",
        "    mask = gif_imread(masks[index])\n",
        "\n",
        "    if transforms is not None:\n",
        "        temp = transforms(image=image, mask=mask)\n",
        "        image = temp[\"image\"]\n",
        "        mask = temp[\"mask\"]\n",
        "\n",
        "    show_examples(name, image, mask)\n",
        "\n",
        "def show_random(images: List[Path], masks: List[Path], transforms=None) -> None:\n",
        "    length = len(images)\n",
        "    index = random.randint(0, length - 1)\n",
        "    show(index, images, masks, transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0zVSTYtk8hf"
      },
      "source": [
        "You can restart the cell below to see more examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0EZVF1pk3tC"
      },
      "outputs": [],
      "source": [
        "show_random(ALL_IMAGES, ALL_MASKS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEVELsNNMTO"
      },
      "source": [
        "The dataset below reads images and masks and optionally applies augmentation to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grrv0FqpY25-"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        images: List[Path],\n",
        "        masks: List[Path] = None,\n",
        "        transforms=None\n",
        "    ) -> None:\n",
        "        self.images = images\n",
        "        self.masks = masks\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        image_path = self.images[idx]\n",
        "        image = utils.imread(image_path)\n",
        "\n",
        "        result = {\"image\": image}\n",
        "\n",
        "        if self.masks is not None:\n",
        "            mask = gif_imread(self.masks[idx])\n",
        "            result[\"mask\"] = mask\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            result = self.transforms(**result)\n",
        "\n",
        "        result[\"filename\"] = image_path.name\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SYZN2FYydbc"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsI3ZS2asQqg"
      },
      "source": [
        "### Augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF0wtzsjNZQ5"
      },
      "source": [
        "[![Albumentation logo](https://albumentations.readthedocs.io/en/latest/_static/logo.png)](https://github.com/albu/albumentations)\n",
        "\n",
        "The [albumentation](https://github.com/albu/albumentations) library works with images and masks at the same time, which is what we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNdK5P0UY26A"
      },
      "outputs": [],
      "source": [
        "import albumentations as albu\n",
        "from albumentations.pytorch import ToTensor\n",
        "\n",
        "\n",
        "def pre_transforms(image_size=224):\n",
        "    return [albu.Resize(image_size, image_size, p=1)]\n",
        "\n",
        "\n",
        "def hard_transforms():\n",
        "    result = [\n",
        "      albu.RandomRotate90(),\n",
        "      albu.Cutout(),\n",
        "      albu.RandomBrightnessContrast(\n",
        "          brightness_limit=0.2, contrast_limit=0.2, p=0.3\n",
        "      ),\n",
        "      albu.GridDistortion(p=0.3),\n",
        "      albu.HueSaturationValue(p=0.3)\n",
        "    ]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def resize_transforms(image_size=224):\n",
        "    BORDER_CONSTANT = 0\n",
        "    pre_size = int(image_size * 1.5)\n",
        "\n",
        "    random_crop = albu.Compose([\n",
        "      albu.SmallestMaxSize(pre_size, p=1),\n",
        "      albu.RandomCrop(\n",
        "          image_size, image_size, p=1\n",
        "      )\n",
        "\n",
        "    ])\n",
        "\n",
        "    rescale = albu.Compose([albu.Resize(image_size, image_size, p=1)])\n",
        "\n",
        "    random_crop_big = albu.Compose([\n",
        "      albu.LongestMaxSize(pre_size, p=1),\n",
        "      albu.RandomCrop(\n",
        "          image_size, image_size, p=1\n",
        "      )\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Converts the image to a square of size image_size x image_size\n",
        "    result = [\n",
        "      albu.OneOf([\n",
        "          random_crop,\n",
        "          rescale,\n",
        "          random_crop_big\n",
        "      ], p=1)\n",
        "    ]\n",
        "\n",
        "    return result\n",
        "\n",
        "def post_transforms():\n",
        "    # we use ImageNet image normalization\n",
        "    # and convert it to torch.Tensor\n",
        "    return [albu.Normalize(), ToTensor()]\n",
        "\n",
        "def compose(transforms_to_compose):\n",
        "    # combine all augmentations into single pipeline\n",
        "    result = albu.Compose([\n",
        "      item for sublist in transforms_to_compose for item in sublist\n",
        "    ])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrRzeFppnPMt"
      },
      "outputs": [],
      "source": [
        "train_transforms = compose([\n",
        "    resize_transforms(),\n",
        "    hard_transforms(),\n",
        "    post_transforms()\n",
        "])\n",
        "valid_transforms = compose([pre_transforms(), post_transforms()])\n",
        "\n",
        "show_transforms = compose([resize_transforms(), hard_transforms()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTiQd-frN74v"
      },
      "source": [
        "Let's look at the augmented results. <br/>\n",
        "You can restart the cell below to see more examples of augmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGIbhBm1orXt"
      },
      "outputs": [],
      "source": [
        "show_random(ALL_IMAGES, ALL_MASKS, transforms=show_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLlx1OvosGX"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGotRWGjyYOw"
      },
      "source": [
        "## Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIfyQYAhY26C"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_loaders(\n",
        "    images: List[Path],\n",
        "    masks: List[Path],\n",
        "    random_state: int,\n",
        "    valid_size: float = 0.2,\n",
        "    batch_size: int = 32,\n",
        "    num_workers: int = 4,\n",
        "    train_transforms_fn = None,\n",
        "    valid_transforms_fn = None,\n",
        ") -> dict:\n",
        "\n",
        "    indices = np.arange(len(images))\n",
        "\n",
        "    # Let's divide the data set into train and valid parts.\n",
        "    train_indices, valid_indices = train_test_split(\n",
        "      indices, test_size=valid_size, random_state=random_state, shuffle=True\n",
        "    )\n",
        "\n",
        "    np_images = np.array(images)\n",
        "    np_masks = np.array(masks)\n",
        "\n",
        "    # Creates our train dataset\n",
        "    train_dataset = SegmentationDataset(\n",
        "      images = np_images[train_indices].tolist(),\n",
        "      masks = np_masks[train_indices].tolist(),\n",
        "      transforms = train_transforms_fn\n",
        "    )\n",
        "\n",
        "    # Creates our valid dataset\n",
        "    valid_dataset = SegmentationDataset(\n",
        "      images = np_images[valid_indices].tolist(),\n",
        "      masks = np_masks[valid_indices].tolist(),\n",
        "      transforms = valid_transforms_fn\n",
        "    )\n",
        "\n",
        "    # Catalyst uses normal torch.data.DataLoader\n",
        "    train_loader = DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      drop_last=True,\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "      valid_dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      drop_last=True,\n",
        "    )\n",
        "\n",
        "    # And excpect to get an OrderedDict of loaders\n",
        "    loaders = collections.OrderedDict()\n",
        "    loaders[\"train\"] = train_loader\n",
        "    loaders[\"valid\"] = valid_loader\n",
        "\n",
        "    return loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG6_pgmkxk_b"
      },
      "outputs": [],
      "source": [
        "if is_fp16_used:\n",
        "    batch_size = 64\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "print(f\"batch_size: {batch_size}\")\n",
        "\n",
        "loaders = get_loaders(\n",
        "    images=ALL_IMAGES,\n",
        "    masks=ALL_MASKS,\n",
        "    random_state=SEED,\n",
        "    train_transforms_fn=train_transforms,\n",
        "    valid_transforms_fn=valid_transforms,\n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFkXsPN9ydbg"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BF2Vgdly9RR"
      },
      "source": [
        "## Experiment\n",
        "### Model\n",
        "\n",
        "Catalyst has [several segmentation models](https://github.com/catalyst-team/catalyst/blob/master/catalyst/contrib/models/segmentation/__init__.py#L16) (Unet, Linknet, FPN, PSPnet and their versions with pretrain from Resnet).\n",
        "\n",
        "> You can read more about them in [our blog post](https://github.com/catalyst-team/catalyst-info#catalyst-info-1-segmentation-models).\n",
        "\n",
        "But for now let's take the model from [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) (SMP for short). The same segmentation architectures have been implemented in this repository, but there are many more pre-trained encoders.\n",
        "\n",
        "[![Segmentation Models logo](https://raw.githubusercontent.com/qubvel/segmentation_models.pytorch/master/pics/logo-small-w300.png)](https://github.com/qubvel/segmentation_models.pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm7JsNrczOQG"
      },
      "outputs": [],
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "# We will use Feature Pyramid Network with pre-trained ResNeXt50 backbone\n",
        "model = smp.FPN(encoder_name=\"resnext50_32x4d\", classes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXxJFBUkybYs"
      },
      "source": [
        "### Model training\n",
        "\n",
        "We will optimize loss as the sum of IoU, Dice and BCE, specifically this function: $IoU + Dice + 0.8*BCE$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhVSEDGbY26G"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "from catalyst.contrib.nn import DiceLoss, IoULoss\n",
        "\n",
        "# we have multiple criterions\n",
        "criterion = {\n",
        "    \"dice\": DiceLoss(),\n",
        "    \"iou\": IoULoss(),\n",
        "    \"bce\": nn.BCEWithLogitsLoss()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IirWWkf8PeXh"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "from catalyst.contrib.nn import RAdam, Lookahead\n",
        "\n",
        "learning_rate = 0.001\n",
        "encoder_learning_rate = 0.0005\n",
        "\n",
        "# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n",
        "layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n",
        "\n",
        "# This function removes weight_decay for biases and applies our layerwise_params\n",
        "model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n",
        "\n",
        "# Catalyst has new SOTA optimizers out of box\n",
        "base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n",
        "optimizer = Lookahead(base_optimizer)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pTmRiOfY26I"
      },
      "outputs": [],
      "source": [
        "from catalyst.dl import SupervisedRunner\n",
        "\n",
        "num_epochs = 3\n",
        "logdir = \"./logs/segmentation\"\n",
        "\n",
        "device = utils.get_device()\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "if is_fp16_used:\n",
        "    fp16_params = dict(opt_level=\"O1\") # params for FP16\n",
        "else:\n",
        "    fp16_params = None\n",
        "\n",
        "print(f\"FP16 params: {fp16_params}\")\n",
        "\n",
        "\n",
        "# by default SupervisedRunner uses \"features\" and \"targets\",\n",
        "# in our case we get \"image\" and \"mask\" keys in dataset __getitem__\n",
        "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5mvw0fO1xI5"
      },
      "source": [
        "### Monitoring in tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7F19YCMydbj"
      },
      "source": [
        "If you do not have a Tensorboard opened after you have run the cell below, try running the cell again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rI7f2Ankydbj"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {logdir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x3Vgswkydbj"
      },
      "source": [
        "### Running train-loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqBlC5_iY26K"
      },
      "outputs": [],
      "source": [
        "from catalyst.dl import DiceCallback, IouCallback, \\\n",
        "  CriterionCallback, MetricAggregationCallback\n",
        "from catalyst.contrib.callbacks import DrawMasksCallback\n",
        "\n",
        "callbacks = [\n",
        "    # Each criterion is calculated separately.\n",
        "    CriterionCallback(\n",
        "        input_key=\"mask\",\n",
        "        prefix=\"loss_dice\",\n",
        "        criterion_key=\"dice\"\n",
        "    ),\n",
        "    CriterionCallback(\n",
        "        input_key=\"mask\",\n",
        "        prefix=\"loss_iou\",\n",
        "        criterion_key=\"iou\"\n",
        "    ),\n",
        "    CriterionCallback(\n",
        "        input_key=\"mask\",\n",
        "        prefix=\"loss_bce\",\n",
        "        criterion_key=\"bce\"\n",
        "    ),\n",
        "\n",
        "    # And only then we aggregate everything into one loss.\n",
        "    MetricAggregationCallback(\n",
        "        prefix=\"loss\",\n",
        "        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n",
        "        # because we want weighted sum, we need to add scale for each loss\n",
        "        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n",
        "    ),\n",
        "\n",
        "    # metrics\n",
        "    DiceCallback(input_key=\"mask\"),\n",
        "    IouCallback(input_key=\"mask\"),\n",
        "    # visualization\n",
        "    DrawMasksCallback(output_key='logits',\n",
        "                      input_image_key='image',\n",
        "                      input_mask_key='mask',\n",
        "                      summary_step=50\n",
        "    )\n",
        "]\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    # our dataloaders\n",
        "    loaders=loaders,\n",
        "    # We can specify the callbacks list for the experiment;\n",
        "    callbacks=callbacks,\n",
        "    # path to save logs\n",
        "    logdir=logdir,\n",
        "    num_epochs=num_epochs,\n",
        "    # save our best checkpoint by IoU metric\n",
        "    main_metric=\"iou\",\n",
        "    # IoU needs to be maximized.\n",
        "    minimize_metric=False,\n",
        "    # for FP16. It uses the variable from the very first cell\n",
        "    fp16=fp16_params,\n",
        "    # prints train logs\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRH8jhg3Q_zB"
      },
      "source": [
        "## Model inference\n",
        "\n",
        "Let's look at the model's predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyugaCieRnx2"
      },
      "outputs": [],
      "source": [
        "TEST_IMAGES = sorted(test_image_path.glob(\"*.jpg\"))\n",
        "\n",
        "# create test dataset\n",
        "test_dataset = SegmentationDataset(\n",
        "    TEST_IMAGES,\n",
        "    transforms=valid_transforms\n",
        ")\n",
        "\n",
        "num_workers: int = 4\n",
        "\n",
        "infer_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "# this get predictions for the whole loader\n",
        "predictions = np.vstack(list(map(\n",
        "    lambda x: x[\"logits\"].cpu().numpy(),\n",
        "    runner.predict_loader(loader=infer_loader, resume=f\"{logdir}/checkpoints/best.pth\")\n",
        ")))\n",
        "\n",
        "print(type(predictions))\n",
        "print(predictions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c57ZjVTxT-9s"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "max_count = 5\n",
        "\n",
        "for i, (features, logits) in enumerate(zip(test_dataset, predictions)):\n",
        "    image = utils.tensor_to_ndimage(features[\"image\"])\n",
        "\n",
        "    mask_ = torch.from_numpy(logits[0]).sigmoid()\n",
        "    mask = utils.detach(mask_ > threshold).astype(\"float\")\n",
        "\n",
        "    show_examples(name=\"\", image=image, mask=mask)\n",
        "\n",
        "    if i >= max_count:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBm6KJg8wTGM"
      },
      "source": [
        "## Model tracing\n",
        "\n",
        "Catalyst allows you to use Runner to make [tracing](https://pytorch.org/docs/stable/jit.html) models.\n",
        "\n",
        "> How to do this in the Config API, we wrote in [our blog (issue \\#2)](https://github.com/catalyst-team/catalyst-info#catalyst-info-2-tracing-with-torchjit)\n",
        "\n",
        "For this purpose it is necessary to pass in a method `trace ` model and a batch on which `predict_batch ` will be executed:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y54paAyPydbm"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(loaders[\"valid\"]))\n",
        "# saves to `logdir` and returns a `ScriptModule` class\n",
        "runner.trace(model=model, batch=batch, logdir=logdir, fp16=is_fp16_used)\n",
        "\n",
        "!ls {logdir}/trace/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3lnr4ojydbn"
      },
      "source": [
        "After this, you can easily load the model and predict anything!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx3M1l1lydbn"
      },
      "outputs": [],
      "source": [
        "from catalyst.utils import trace\n",
        "\n",
        "if is_fp16_used:\n",
        "    model = trace.load_traced_model(\n",
        "        f\"{logdir}/trace/traced-forward-opt_O1.pth\",\n",
        "        device=\"cuda\",\n",
        "        opt_level=\"O1\"\n",
        "    )\n",
        "else:\n",
        "    model = trace.load_traced_model(\n",
        "        f\"{logdir}/trace/traced-forward.pth\",\n",
        "        device=\"cpu\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDD-Ldv_ydbo"
      },
      "outputs": [],
      "source": [
        "model_input = batch[\"image\"].to(\"cuda\" if is_fp16_used else \"cpu\")\n",
        "model(model_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41YoixbWydbo"
      },
      "source": [
        "### Advanced: Custom Callbacks\n",
        "\n",
        "Let's plot the heatmap of predicted masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "at-4G3PDydbp"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "from catalyst.dl import Callback, CallbackOrder, IRunner\n",
        "\n",
        "\n",
        "class CustomInferCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__(CallbackOrder.Internal)\n",
        "        self.heatmap = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def on_loader_start(self, runner: IRunner):\n",
        "        self.predictions = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def on_batch_end(self, runner: IRunner):\n",
        "        # data from the Dataloader\n",
        "        # image, mask = runner.input[\"image\"], runner.input[\"mask\"]\n",
        "        logits = runner.output[\"logits\"]\n",
        "        probabilities = torch.sigmoid(logits)\n",
        "\n",
        "        self.heatmap = (\n",
        "            probabilities\n",
        "            if self.heatmap is None\n",
        "            else self.heatmap + probabilities\n",
        "        )\n",
        "        self.counter += len(probabilities)\n",
        "\n",
        "    def on_loader_end(self, runner: IRunner):\n",
        "        self.heatmap = self.heatmap.sum(axis=0)\n",
        "        self.heatmap /= self.counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zQ479iHOydbp"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from catalyst.dl import CheckpointCallback\n",
        "\n",
        "\n",
        "infer_loaders = {\"infer\": loaders[\"valid\"]}\n",
        "model = smp.FPN(encoder_name=\"resnext50_32x4d\", classes=1)\n",
        "\n",
        "device = utils.get_device()\n",
        "if is_fp16_used:\n",
        "    fp16_params = dict(opt_level=\"O1\") # params for FP16\n",
        "else:\n",
        "    fp16_params = None\n",
        "\n",
        "runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")\n",
        "runner.infer(\n",
        "    model=model,\n",
        "    loaders=infer_loaders,\n",
        "    callbacks=OrderedDict([\n",
        "        (\"loader\", CheckpointCallback(resume=f\"{logdir}/checkpoints/best.pth\")),\n",
        "        (\"infer\", CustomInferCallback())\n",
        "    ]),\n",
        "    fp16=fp16_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zr8cgR2dydbp"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "heatmap = utils.detach(runner.runner.callbacks[\"infer\"].heatmap[0])\n",
        "plt.figure(figsize=(20, 9))\n",
        "plt.imshow(heatmap, cmap=\"hot\", interpolation=\"nearest\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OvmXABrnydbq"
      },
      "source": [
        "### Advanced: test-time augmentations (TTA)\n",
        "\n",
        "There is [ttach](https://github.com/qubvel/ttach) is a new awesome library for test-time augmentation for segmentation or classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pnrCqQuYydbq"
      },
      "outputs": [],
      "source": [
        "import ttach as tta\n",
        "\n",
        "# D4 makes horizontal and vertical flips + rotations for [0, 90, 180, 270] angels.\n",
        "# and then merges the result masks with merge_mode=\"mean\"\n",
        "tta_model = tta.SegmentationTTAWrapper(model, tta.aliases.d4_transform(), merge_mode=\"mean\")\n",
        "\n",
        "tta_runner = SupervisedRunner(\n",
        "    model=tta_model,\n",
        "    device=utils.get_device(),\n",
        "    input_key=\"image\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5t4n-uMpydbq"
      },
      "outputs": [],
      "source": [
        "infer_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "batch = next(iter(infer_loader))\n",
        "\n",
        "# predict_batch will automatically move the batch to the Runner's device\n",
        "tta_predictions = tta_runner.predict_batch(batch)\n",
        "\n",
        "# shape is `batch_size x channels x height x width`\n",
        "print(tta_predictions[\"logits\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA5YbW9Uydbr"
      },
      "source": [
        "Let's see our mask after TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0Mr7o-pSydbr"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "\n",
        "image = utils.tensor_to_ndimage(batch[\"image\"][0])\n",
        "\n",
        "mask_ = tta_predictions[\"logits\"][0, 0].sigmoid()\n",
        "mask = utils.detach(mask_ > threshold).astype(\"float\")\n",
        "\n",
        "show_examples(name=\"\", image=image, mask=mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckZPctJHydbs"
      },
      "outputs": [],
      "source": []
    }
  ]
}